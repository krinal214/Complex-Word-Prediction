{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport spacy\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import cmudict\nfrom collections import defaultdict\nimport math\nimport torch\nimport transformers\nfrom transformers import BertTokenizer,BertModel\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:55:07.136913Z","iopub.execute_input":"2022-07-28T15:55:07.137582Z","iopub.status.idle":"2022-07-28T15:55:07.143693Z","shell.execute_reply.started":"2022-07-28T15:55:07.137548Z","shell.execute_reply":"2022-07-28T15:55:07.142646Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import re\nimport warnings\nfrom string import punctuation\n\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.util import ngrams\nclass SyllableTokenizer(TokenizerI):\n    \"\"\"\n    Syllabifies words based on the Sonority Sequencing Principle (SSP).\n\n        >>> from nltk.tokenize import SyllableTokenizer\n        >>> from nltk import word_tokenize\n        >>> SSP = SyllableTokenizer()\n        >>> SSP.tokenize('justification')\n        ['jus', 'ti', 'fi', 'ca', 'tion']\n        >>> text = \"This is a foobar-like sentence.\"\n        >>> [SSP.tokenize(token) for token in word_tokenize(text)]\n        [['This'], ['is'], ['a'], ['foo', 'bar', '-', 'li', 'ke'], ['sen', 'ten', 'ce'], ['.']]\n    \"\"\"\n\n    def __init__(self, lang=\"en\", sonority_hierarchy=False):\n        \"\"\"\n        :param lang: Language parameter, default is English, 'en'\n        :type lang: str\n        :param sonority_hierarchy: Sonority hierarchy according to the\n                                   Sonority Sequencing Principle.\n        :type sonority_hierarchy: list(str)\n        \"\"\"\n        # Sonority hierarchy should be provided in descending order.\n        # If vowels are spread across multiple levels, they should be\n        # passed assigned self.vowels var together, otherwise should be\n        # placed in first index of hierarchy.\n        if not sonority_hierarchy and lang == \"en\":\n            sonority_hierarchy = [\n                \"aeiouy\",  # vowels.\n                \"lmnrw\",  # nasals.\n                \"zvsf\",  # fricatives.\n                \"bcdgtkpqxhj\",  # stops.\n            ]\n\n        self.vowels = sonority_hierarchy[0]\n        self.phoneme_map = {}\n        for i, level in enumerate(sonority_hierarchy):\n            for c in level:\n                sonority_level = len(sonority_hierarchy) - i\n                self.phoneme_map[c] = sonority_level\n                self.phoneme_map[c.upper()] = sonority_level\n\n    def assign_values(self, token):\n        \"\"\"\n        Assigns each phoneme its value from the sonority hierarchy.\n        Note: Sentence/text has to be tokenized first.\n\n        :param token: Single word or token\n        :type token: str\n        :return: List of tuples, first element is character/phoneme and\n                 second is the soronity value.\n        :rtype: list(tuple(str, int))\n        \"\"\"\n        syllables_values = []\n        for c in token:\n            try:\n                syllables_values.append((c, self.phoneme_map[c]))\n            except KeyError:\n                if c not in punctuation:\n                    warnings.warn(\n                        \"Character not defined in sonority_hierarchy,\"\n                        \" assigning as vowel: '{}'\".format(c)\n                    )\n                    syllables_values.append((c, max(self.phoneme_map.values())))\n                    self.vowels += c\n                else:  # If it's a punctuation, assing -1.\n                    syllables_values.append((c, -1))\n        return syllables_values\n\n\n    def validate_syllables(self, syllable_list):\n        \"\"\"\n        Ensures each syllable has at least one vowel.\n        If the following syllable doesn't have vowel, add it to the current one.\n\n        :param syllable_list: Single word or token broken up into syllables.\n        :type syllable_list: list(str)\n        :return: Single word or token broken up into syllables\n                 (with added syllables if necessary)\n        :rtype: list(str)\n        \"\"\"\n        valid_syllables = []\n        front = \"\"\n        for i, syllable in enumerate(syllable_list):\n            if syllable in punctuation:\n                valid_syllables.append(syllable)\n                continue\n            if not re.search(\"|\".join(self.vowels), syllable):\n                if len(valid_syllables) == 0:\n                    front += syllable\n                else:\n                    valid_syllables = valid_syllables[:-1] + [\n                        valid_syllables[-1] + syllable\n                    ]\n            else:\n                if len(valid_syllables) == 0:\n                    valid_syllables.append(front + syllable)\n                else:\n                    valid_syllables.append(syllable)\n\n        return valid_syllables\n\n\n    def tokenize(self, token):\n        \"\"\"\n        Apply the SSP to return a list of syllables.\n        Note: Sentence/text has to be tokenized first.\n\n        :param token: Single word or token\n        :type token: str\n        :return syllable_list: Single word or token broken up into syllables.\n        :rtype: list(str)\n        \"\"\"\n        # assign values from hierarchy\n        syllables_values = self.assign_values(token)\n\n        # if only one vowel return word\n        if sum(token.count(x) for x in self.vowels) <= 1:\n            return [token]\n\n        syllable_list = []\n        syllable = syllables_values[0][0]  # start syllable with first phoneme\n        for trigram in ngrams(syllables_values, n=3):\n            phonemes, values = zip(*trigram)\n            # Sonority of previous, focal and following phoneme\n            prev_value, focal_value, next_value = values\n            # Focal phoneme.\n            focal_phoneme = phonemes[1]\n\n            # These cases trigger syllable break.\n            if focal_value == -1:  # If it's a punctuation, just break.\n                syllable_list.append(syllable)\n                syllable_list.append(focal_phoneme)\n                syllable = \"\"\n            elif prev_value >= focal_value == next_value:\n                syllable += focal_phoneme\n                syllable_list.append(syllable)\n                syllable = \"\"\n\n            elif prev_value > focal_value < next_value:\n                syllable_list.append(syllable)\n                syllable = \"\"\n                syllable += focal_phoneme\n\n            # no syllable break\n            else:\n                syllable += focal_phoneme\n\n        syllable += syllables_values[-1][0]  # append last phoneme\n        syllable_list.append(syllable)\n\n        return self.validate_syllables(syllable_list)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:55:08.250738Z","iopub.execute_input":"2022-07-28T15:55:08.251156Z","iopub.status.idle":"2022-07-28T15:55:08.274515Z","shell.execute_reply.started":"2022-07-28T15:55:08.251122Z","shell.execute_reply":"2022-07-28T15:55:08.273627Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Features:\n    emmbed_dict = {}\n    with open('../input/glove300/glove.42B.300d.txt','r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:],'float32')\n            emmbed_dict[word]=vector\n    tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n    model=BertModel.from_pretrained('bert-base-uncased')\n    \n    def __init__(self,path):\n        self.df=pd.read_csv(path,encoding='utf-8', delimiter='\\t', quotechar='\\t', keep_default_na=False)\n        self.df.rename(columns = {'subcorpus':'corpus'}, inplace = True)\n        self.maxlen=512\n        \n                \n    def remove_whitespace(self,text):\n        text=text.lower().strip()\n        return \" \".join(text.split());\n    \n    def remove_punct(self,text):\n        tokenizer=RegexpTokenizer(r\"\\w+\")\n        text=tokenizer.tokenize(text)\n        return \" \".join(text)\n\n    def remove_stopword(self,text):\n        text=text.split()\n        res=[]\n        en_stopwords=stopwords.words('english')\n        for t in text:\n            if t not in en_stopwords:\n                res.append(t)\n        return \" \".join(res)\n    \n    def synonyms(self,word):\n        lis=word.split()\n        cnt=0\n        for t in lis:\n            synonyms = []\n            for syn in wordnet.synsets(t): \n                for l in syn.lemmas(): \n                    synonyms.append(l.name())\n            synonyms = set(synonyms)\n            cnt+=len(synonyms)\n\n        return cnt/len(lis)\n\n    def antonyms(self,word):\n        lis=word.split()\n        cnt=0\n        for t in lis:\n            antonyms = [] \n            for syn in wordnet.synsets(word): \n                for l in syn.lemmas(): \n                    if l.antonyms(): \n                        antonyms.append(l.antonyms()[0].name()) \n\n            antonyms = set(antonyms)    \n            cnt+=len(antonyms)\n        return cnt/len(lis)\n\n    def hypernyms(self,word):\n        hypernyms=0\n        try:\n            results = wordnet.synsets(word)\n            hypernyms = len(results[0].hypernyms())\n            return hypernyms\n        except:\n            return hypernyms\n\n\n    def hyponyms(self,word):\n        lis=word.split()\n        cnt=0\n        for t in lis:\n            hyponyms=0\n            try:\n                results = wordnet.synsets(t)\n            except:\n                cnt+=hyponyms\n                continue\n            try:\n                hyponyms = len(results[0].hyponyms())\n                cnt+=hyponyms\n                continue\n            except:\n                cnt+=hyponyms\n                continue\n        return cnt/len(lis)\n\n\n\n    def clean(self):\n        self.df.replace(to_replace= np.nan, value = \"null\", inplace=True)\n        self.df['sentence']=self.df['sentence'].apply(self.remove_whitespace)\n        self.df['sentence']=self.df['sentence'].apply(self.remove_punct)\n        self.df['token']=self.df['token'].apply(self.remove_whitespace)\n    \n    def tf(self):\n        res=[]\n        token_freq = {}\n        token_freq = defaultdict(lambda:0,token_freq)\n        \n        for sen in self.df['sentence']:\n            sp=sen.split()\n            for s in sp:\n                token_freq[s]+=1\n        for t in self.df['token']:\n            lis=t.split()\n            val=0\n            for l in lis:\n                val+=math.log10(1+token_freq[l])\n            res.append(val)\n\n        return res\n    \n    def vowels(self):\n        res=[]\n        vowel=['a','e','i','o','u']\n        for token in self.df['token']:\n            c=0\n            for s in token:\n                if s in vowel:\n                    c+=1\n            res.append(c)\n        return res\n    \n    def token_len(self):\n        res=[]\n        for t in self.df['token']:\n            res.append(len(t))\n        return res\n\n\n    def syllable_count(self):\n        d = cmudict.dict()\n        res=[]\n        for word in self.df['token']:\n            val=0\n            lis=word.split()\n            for l in lis:\n                try:\n                    val+=[len(list(y for y in x if y[-1].isdigit())) for x in d[l.lower()]][0]\n                except:\n                    st=SyllableTokenizer()\n                    val+=len(st.tokenize(l))\n            res.append(val+len(lis))\n        return res\n    \n    def pos_tagger(self):\n        tagger=spacy.load('en_core_web_sm')\n        length=self.df.shape[0]\n        pos_tags={'NOUN':[0]*length,'ADJ':[0]*length,'VERB':[0]*length,'ADV':[0]*length}\n        \n        for index, row in self.df.iterrows():\n            sen=tagger(row['sentence'])\n            lis=row['token'].split()\n            for i in range(len(sen)):\n                if sen[i].text in lis and sen[i].pos_ in pos_tags:\n                    pos_tags[sen[i].pos_][index]+=1\n    \n        temp_df=pd.DataFrame.from_dict(pos_tags)\n        df=pd.concat([self.df,temp_df],axis=1)\n        return df\n    \n    def glove_token(self,token):\n        if token in Features.emmbed_dict:\n            return Features.emmbed_dict[token]\n        else:\n            return np.zeros((300,))\n        \n    def glove_embedding(self):\n        res=[]\n        for t in self.df['token']:\n            lis=t.split()\n            ans=np.zeros((300,))\n            for l in lis:\n                ans+=self.glove_token(l)\n            res.append(ans/len(lis))\n        return np.array(res)\n    \n    def bert_no_of_tokens(self):\n        res=[]\n        for t in self.df['token']:\n            lis=t.split()\n            v=0\n            for l in lis:\n                v+=len(Features.tokenizer.tokenize(t))\n            res.append(v)\n        return res\n    \n    def index_of_token(self,text,token):\n        tokens=Features.tokenizer.tokenize(text)\n        res=[]\n        lis=token.split()\n        for t in lis:\n            s=\"\"\n            ids=[]\n            i=0\n            while i<len(tokens) and i<self.maxlen-2:\n                if t.startswith(tokens[i]):\n                    s=tokens[i]\n                    ids.append(i)\n                    i+=1\n                    while i<len(tokens) and i<self.maxlen-2 and tokens[i].startswith('##') and t.startswith(s+tokens[i][2:]):\n                        s+=tokens[i][2:]\n                        ids.append(i)\n                        i+=1\n                    if s==t:\n                        break\n                    else:\n                        s=\"\"\n                        ids.clear()\n                else:\n                    i+=1\n            res+=ids\n        return res\n    \n    def contextual_features(self):\n        res1=[]\n        res2=[]\n        DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        Features.model=Features.model.to(DEVICE)\n        Features.model.eval()\n        for i,row in self.df.iterrows():\n            with torch.no_grad():\n                emb=Features.tokenizer(row['sentence'],return_tensors='pt',truncation=True,max_length=512).to(DEVICE)\n                pred=Features.model(input_ids=emb['input_ids'],attention_mask=emb['attention_mask'],output_hidden_states=True)\n                emb_token=torch.zeros(1,768).to(DEVICE)\n                ids=self.index_of_token(row['sentence'],row['token'])\n                if len(ids)==0:\n                    res1.append(emb_token)\n                else:\n                    for idx in ids:\n                        emb_token+=pred.hidden_states[-2][0][idx+1].reshape(1,768)\n                    emb_token/=len(ids)\n                    res1.append(emb_token)\n            res2.append(pred.pooler_output)\n            \n        res1=torch.cat(res1)\n        res1=np.array(res1.detach().cpu())\n        res2=torch.cat(res2)\n        res2=np.array(res2.detach().cpu())\n        \n        return np.hstack((res1,res2))\n                    \n    def contextual_features0(self):\n        res1=[]\n        res2=[]\n        DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        Features.model=Features.model.to(DEVICE)\n        Features.model.eval()\n        for i,row in self.df.iterrows():\n            with torch.no_grad():\n                emb=Features.tokenizer(row['sentence'],return_tensors='pt',truncation=True,max_length=512).to(DEVICE)\n                pred=Features.model(input_ids=emb['input_ids'],attention_mask=emb['attention_mask'],output_hidden_states=True)\n                emb_token=torch.zeros(1,768).to(DEVICE)\n                ids=self.index_of_token(row['sentence'],row['token'])\n                if len(ids)==0:\n                    emb_token=torch.hstack((emb_token,emb_token,emb_token))\n                elif len(ids)==1:\n                    v1=pred.hidden_states[-2][0][ids[0]+1].reshape(1,768)\n                    emb_token=torch.hstack((v1,emb_token,emb_token))\n                elif len(ids)==2:\n                    v1=pred.hidden_states[-2][0][ids[0]+1].reshape(1,768)\n                    v2=pred.hidden_states[-2][0][ids[1]+1].reshape(1,768)\n                    emb_token=torch.hstack((v1,v2,emb_token))\n                else:\n                    v1=pred.hidden_states[-2][0][ids[0]+1].reshape(1,768)\n                    v2=pred.hidden_states[-2][0][ids[1]+1].reshape(1,768)\n                    v3=pred.hidden_states[-2][0][ids[2]+1].reshape(1,768)\n                    emb_token=torch.hstack((v1,v2,v3))\n\n            res1.append(emb_token)\n            res2.append(pred.pooler_output)\n        res1=torch.cat(res1)\n        res1=np.array(res1.detach().cpu())\n        res2=torch.cat(res2)\n        res2=np.array(res2.detach().cpu())\n        \n        return np.hstack((res1,res2))\n    \n    def preprocess(self):\n        self.clean()\n        self.df['synonyms'] = self.df['token'].apply(self.synonyms)\n        self.df['antonyms'] = self.df['token'].apply(self.antonyms)\n        #self.df['hypernyms'] = self.df['token'].apply(self.hypernyms)\n        self.df['hyponyms'] = self.df['token'].apply(self.hyponyms)\n        self.df['vowels']=self.vowels()\n        self.df['token_len']=self.token_len()\n        self.df['tf']=self.tf()\n        self.df['syllable']=self.syllable_count()\n        self.df=self.pos_tagger()\n        self.df['bert_token_count']=self.bert_no_of_tokens()\n        dummy=pd.get_dummies(self.df['corpus'],drop_first=True,prefix='corpus_')\n        df_y=self.df['complexity']\n        df_x=self.df.drop(columns=['corpus','id','sentence','token','complexity'])\n        df_x = pd.concat([df_x, dummy],axis=1)\n        np_vectors=np.hstack((self.glove_embedding(),self.contextual_features()))\n        temp_df=pd.DataFrame(np_vectors)\n        df_x=pd.concat([df_x,temp_df],axis=1)\n        return pd.concat([df_x,df_y],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:55:09.293863Z","iopub.execute_input":"2022-07-28T15:55:09.294224Z","iopub.status.idle":"2022-07-28T15:58:08.382454Z","shell.execute_reply.started":"2022-07-28T15:55:09.294192Z","shell.execute_reply":"2022-07-28T15:58:08.381364Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4401d4d0989a473ba29214a6ba32af32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab29c08ce244ba58930d8c7de025aba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625e5ee4254d483d9e7a92cf005c11a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c26ab4b89c741119a93740d319358d0"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"multi_trail=Features('../input/lcp-trail/lcp_multi_trial.tsv').preprocess()\nmulti_trail.to_csv('multi_trail.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T15:58:08.384671Z","iopub.execute_input":"2022-07-28T15:58:08.385120Z","iopub.status.idle":"2022-07-28T15:58:22.160162Z","shell.execute_reply.started":"2022-07-28T15:58:08.385076Z","shell.execute_reply":"2022-07-28T15:58:22.158732Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"multi_train=Features('../input/lcp-train/lcp_multi_train.tsv').preprocess()\nmulti_train.to_csv('multi_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T16:02:10.852723Z","iopub.execute_input":"2022-07-28T16:02:10.853424Z","iopub.status.idle":"2022-07-28T16:02:58.110996Z","shell.execute_reply.started":"2022-07-28T16:02:10.853386Z","shell.execute_reply":"2022-07-28T16:02:58.110024Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"multi_test=Features('../input/lcp-test/lcp_multi_test.tsv').preprocess()\nmulti_test.to_csv('multi_test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T16:02:58.113222Z","iopub.execute_input":"2022-07-28T16:02:58.113632Z","iopub.status.idle":"2022-07-28T16:03:04.457995Z","shell.execute_reply.started":"2022-07-28T16:02:58.113596Z","shell.execute_reply":"2022-07-28T16:03:04.457039Z"},"trusted":true},"execution_count":9,"outputs":[]}]}