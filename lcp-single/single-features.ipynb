{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport spacy\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import cmudict\nfrom collections import defaultdict\nimport math\nimport torch\nimport transformers\nfrom transformers import BertTokenizer,BertModel\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:08.444964Z","iopub.execute_input":"2022-07-27T18:47:08.445518Z","iopub.status.idle":"2022-07-27T18:47:20.296921Z","shell.execute_reply.started":"2022-07-27T18:47:08.445401Z","shell.execute_reply":"2022-07-27T18:47:20.295448Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"nltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:42:28.021125Z","iopub.execute_input":"2022-07-27T17:42:28.021605Z","iopub.status.idle":"2022-07-27T17:42:28.092567Z","shell.execute_reply.started":"2022-07-27T17:42:28.021564Z","shell.execute_reply":"2022-07-27T17:42:28.091222Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport warnings\nfrom string import punctuation\n\nfrom nltk.tokenize.api import TokenizerI\nfrom nltk.util import ngrams\nclass SyllableTokenizer(TokenizerI):\n    \"\"\"\n    Syllabifies words based on the Sonority Sequencing Principle (SSP).\n\n        >>> from nltk.tokenize import SyllableTokenizer\n        >>> from nltk import word_tokenize\n        >>> SSP = SyllableTokenizer()\n        >>> SSP.tokenize('justification')\n        ['jus', 'ti', 'fi', 'ca', 'tion']\n        >>> text = \"This is a foobar-like sentence.\"\n        >>> [SSP.tokenize(token) for token in word_tokenize(text)]\n        [['This'], ['is'], ['a'], ['foo', 'bar', '-', 'li', 'ke'], ['sen', 'ten', 'ce'], ['.']]\n    \"\"\"\n\n    def __init__(self, lang=\"en\", sonority_hierarchy=False):\n        \"\"\"\n        :param lang: Language parameter, default is English, 'en'\n        :type lang: str\n        :param sonority_hierarchy: Sonority hierarchy according to the\n                                   Sonority Sequencing Principle.\n        :type sonority_hierarchy: list(str)\n        \"\"\"\n        # Sonority hierarchy should be provided in descending order.\n        # If vowels are spread across multiple levels, they should be\n        # passed assigned self.vowels var together, otherwise should be\n        # placed in first index of hierarchy.\n        if not sonority_hierarchy and lang == \"en\":\n            sonority_hierarchy = [\n                \"aeiouy\",  # vowels.\n                \"lmnrw\",  # nasals.\n                \"zvsf\",  # fricatives.\n                \"bcdgtkpqxhj\",  # stops.\n            ]\n\n        self.vowels = sonority_hierarchy[0]\n        self.phoneme_map = {}\n        for i, level in enumerate(sonority_hierarchy):\n            for c in level:\n                sonority_level = len(sonority_hierarchy) - i\n                self.phoneme_map[c] = sonority_level\n                self.phoneme_map[c.upper()] = sonority_level\n\n    def assign_values(self, token):\n        \"\"\"\n        Assigns each phoneme its value from the sonority hierarchy.\n        Note: Sentence/text has to be tokenized first.\n\n        :param token: Single word or token\n        :type token: str\n        :return: List of tuples, first element is character/phoneme and\n                 second is the soronity value.\n        :rtype: list(tuple(str, int))\n        \"\"\"\n        syllables_values = []\n        for c in token:\n            try:\n                syllables_values.append((c, self.phoneme_map[c]))\n            except KeyError:\n                if c not in punctuation:\n                    warnings.warn(\n                        \"Character not defined in sonority_hierarchy,\"\n                        \" assigning as vowel: '{}'\".format(c)\n                    )\n                    syllables_values.append((c, max(self.phoneme_map.values())))\n                    self.vowels += c\n                else:  # If it's a punctuation, assing -1.\n                    syllables_values.append((c, -1))\n        return syllables_values\n\n\n    def validate_syllables(self, syllable_list):\n        \"\"\"\n        Ensures each syllable has at least one vowel.\n        If the following syllable doesn't have vowel, add it to the current one.\n\n        :param syllable_list: Single word or token broken up into syllables.\n        :type syllable_list: list(str)\n        :return: Single word or token broken up into syllables\n                 (with added syllables if necessary)\n        :rtype: list(str)\n        \"\"\"\n        valid_syllables = []\n        front = \"\"\n        for i, syllable in enumerate(syllable_list):\n            if syllable in punctuation:\n                valid_syllables.append(syllable)\n                continue\n            if not re.search(\"|\".join(self.vowels), syllable):\n                if len(valid_syllables) == 0:\n                    front += syllable\n                else:\n                    valid_syllables = valid_syllables[:-1] + [\n                        valid_syllables[-1] + syllable\n                    ]\n            else:\n                if len(valid_syllables) == 0:\n                    valid_syllables.append(front + syllable)\n                else:\n                    valid_syllables.append(syllable)\n\n        return valid_syllables\n\n\n    def tokenize(self, token):\n        \"\"\"\n        Apply the SSP to return a list of syllables.\n        Note: Sentence/text has to be tokenized first.\n\n        :param token: Single word or token\n        :type token: str\n        :return syllable_list: Single word or token broken up into syllables.\n        :rtype: list(str)\n        \"\"\"\n        # assign values from hierarchy\n        syllables_values = self.assign_values(token)\n\n        # if only one vowel return word\n        if sum(token.count(x) for x in self.vowels) <= 1:\n            return [token]\n\n        syllable_list = []\n        syllable = syllables_values[0][0]  # start syllable with first phoneme\n        for trigram in ngrams(syllables_values, n=3):\n            phonemes, values = zip(*trigram)\n            # Sonority of previous, focal and following phoneme\n            prev_value, focal_value, next_value = values\n            # Focal phoneme.\n            focal_phoneme = phonemes[1]\n\n            # These cases trigger syllable break.\n            if focal_value == -1:  # If it's a punctuation, just break.\n                syllable_list.append(syllable)\n                syllable_list.append(focal_phoneme)\n                syllable = \"\"\n            elif prev_value >= focal_value == next_value:\n                syllable += focal_phoneme\n                syllable_list.append(syllable)\n                syllable = \"\"\n\n            elif prev_value > focal_value < next_value:\n                syllable_list.append(syllable)\n                syllable = \"\"\n                syllable += focal_phoneme\n\n            # no syllable break\n            else:\n                syllable += focal_phoneme\n\n        syllable += syllables_values[-1][0]  # append last phoneme\n        syllable_list.append(syllable)\n\n        return self.validate_syllables(syllable_list)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:20.299431Z","iopub.execute_input":"2022-07-27T18:47:20.300519Z","iopub.status.idle":"2022-07-27T18:47:20.325520Z","shell.execute_reply.started":"2022-07-27T18:47:20.300476Z","shell.execute_reply":"2022-07-27T18:47:20.324063Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Features:\n    emmbed_dict = {}\n    with open('../input/glove300/glove.42B.300d.txt','r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:],'float32')\n            emmbed_dict[word]=vector\n    tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n    model=BertModel.from_pretrained('bert-base-uncased')\n    \n    def __init__(self,path):\n        self.df=pd.read_csv(path,encoding='utf-8', delimiter='\\t', quotechar='\\t', keep_default_na=False)\n        self.df.rename(columns = {'subcorpus':'corpus'}, inplace = True)\n        self.maxlen=512\n        \n                \n    def remove_whitespace(self,text):\n        text=text.lower().strip()\n        return \" \".join(text.split());\n    \n    def remove_punct(self,text):\n        tokenizer=RegexpTokenizer(r\"\\w+\")\n        text=tokenizer.tokenize(text)\n        return \" \".join(text)\n\n    def remove_stopword(self,text):\n        text=text.split()\n        res=[]\n        en_stopwords=stopwords.words('english')\n        for t in text:\n            if t not in en_stopwords:\n                res.append(t)\n        return \" \".join(res)\n    \n    def synonyms(self,word):\n        synonyms = [] \n        for syn in wordnet.synsets(word): \n            for l in syn.lemmas(): \n                synonyms.append(l.name()) \n\n        synonyms = set(synonyms)\n\n        return len(synonyms)\n\n    def antonyms(self,word):\n        antonyms = [] \n        for syn in wordnet.synsets(word): \n            for l in syn.lemmas(): \n                if l.antonyms(): \n                    antonyms.append(l.antonyms()[0].name()) \n\n        antonyms = set(antonyms)    \n        return len(antonyms)\n\n    def hypernyms(self,word):\n        hypernyms=0\n        try:\n            results = wordnet.synsets(word)\n            hypernyms = len(results[0].hypernyms())\n            return hypernyms\n        except:\n            return hypernyms\n\n\n    def hyponyms(self,word):\n        hyponyms=0\n        try:\n            results = wordnet.synsets(word)\n        except:\n            return hyponyms\n        try:\n            hyponyms = len(results[0].hyponyms())\n            return hyponyms\n        except:\n            return hyponyms\n\n\n\n    def clean(self):\n        self.df.replace(to_replace= np.nan, value = \"null\", inplace=True)\n        self.df['sentence']=self.df['sentence'].apply(self.remove_whitespace)\n        self.df['sentence']=self.df['sentence'].apply(self.remove_punct)\n        self.df['token']=self.df['token'].apply(self.remove_whitespace)\n    \n    def tf(self):\n        res=[]\n        token_freq = {}\n        token_freq = defaultdict(lambda:0,token_freq)\n        \n        for sen in self.df['sentence']:\n            sp=sen.split()\n            for s in sp:\n                token_freq[s]+=1\n        for t in self.df['token']:\n            val=0\n            val=math.log10(1+token_freq[t])\n            res.append(val)\n\n        return res\n    \n    def vowels(self):\n        res=[]\n        vowel=['a','e','i','o','u']\n        for token in self.df['token']:\n            c=0\n            for s in token:\n                if s in vowel:\n                    c+=1\n            res.append(c)\n        return res\n    \n    def token_len(self):\n        res=[]\n        for t in self.df['token']:\n            res.append(len(t))\n        return res\n\n\n    def syllable_count(self):\n        d = cmudict.dict()\n        res=[]\n        for word in self.df['token']:\n            val=0\n            try:\n                val+=[len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n            except:\n                st=SyllableTokenizer()\n                val+=len(st.tokenize(word))\n            res.append(val)\n        return res\n    \n    def pos_tagger(self):\n        tagger=spacy.load('en_core_web_sm')\n        length=self.df.shape[0]\n        pos_tags={'NOUN':[0]*length,'ADJ':[0]*length,'VERB':[0]*length,'ADV':[0]*length}\n        res=[]\n        for index, row in self.df.iterrows():\n            sen=tagger(row['sentence'])\n            for i in range(len(sen)):\n                if sen[i].text==row['token'] and sen[i].pos_ in pos_tags:\n                    pos_tags[sen[i].pos_][index]+=1\n                    break\n        temp_df=pd.DataFrame.from_dict(pos_tags)\n        df=pd.concat([self.df,temp_df],axis=1)\n        return df\n\n    \n    def pos_tagger1(self):\n        tagger=spacy.load('en_core_web_sm')\n        pos_tags={'NOUN':11,'ADJ':10,'VERB':9,'ADV':8,'ADP':7,'CONJ':6,'CCONJ':5,'AUX':4,'DET':3,'PRON':2,'SCONJ':1}\n        pos_tags=defaultdict(lambda:0,pos_tags)\n        res=[]\n        for index, row in self.df.iterrows():\n            sen=tagger(row['sentence'])\n            f=False\n            for i in range(len(sen)):\n                if sen[i].text==row['token']:\n                    cur=pos_tags[sen[i].pos_]\n                    res.append(cur)\n                    f=True\n\n                    break\n            if not f:\n                res.append(0)\n        return res\n    \n    def glove_token(self,token):\n        if token in Features.emmbed_dict:\n            return Features.emmbed_dict[token]\n        else:\n            return np.zeros((300,))\n        \n    def glove_embedding(self):\n        res=[]\n        for t in self.df['token']:\n            ans=np.zeros((300,))\n            ans+=self.glove_token(t)\n            res.append(ans)\n        return np.array(res)\n    \n    def bert_no_of_tokens(self):\n        res=[]\n        for t in self.df['token']:\n            res.append(len(Features.tokenizer.tokenize(t)))\n        return res\n    \n    def index_of_token(self,text,token):\n        tokens=Features.tokenizer.tokenize(text)\n        ids=[]\n        s=\"\"\n        i=0\n        while i<len(tokens) and i<self.maxlen-2:\n            if token.startswith(tokens[i]):\n                s=tokens[i]\n                ids.append(i)\n                i+=1\n                while i<len(tokens) and i<self.maxlen-2 and tokens[i].startswith('##') and token.startswith(s+tokens[i][2:]):\n                    s+=tokens[i][2:]\n                    ids.append(i)\n                    i+=1\n                if s==token:\n                    break\n                else:\n                    s=\"\"\n                    ids.clear()\n            else:\n                i+=1\n        return ids\n    \n    def contextual_features(self):\n        res1=[]\n        res2=[]\n        DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        Features.model=Features.model.to(DEVICE)\n        Features.model.eval()\n        for i,row in self.df.iterrows():\n            with torch.no_grad():\n                emb=Features.tokenizer(row['sentence'],return_tensors='pt',truncation=True,max_length=512).to(DEVICE)\n                pred=Features.model(input_ids=emb['input_ids'],attention_mask=emb['attention_mask'],output_hidden_states=True)\n                emb_token=torch.zeros(1,768).to(DEVICE)\n                ids=self.index_of_token(row['sentence'],row['token'])\n                if len(ids)==0:\n                    emb_token=torch.hstack((emb_token,emb_token,emb_token))\n                elif len(ids)==1:\n                    v1=pred.hidden_states[-2][0][ids[0]+1].reshape(1,768)\n                    emb_token=torch.hstack((v1,emb_token,emb_token))\n                elif len(ids)==2:\n                    v1=pred.hidden_states[-2][0][ids[0]+1].reshape(1,768)\n                    v2=pred.hidden_states[-2][0][ids[1]+1].reshape(1,768)\n                    emb_token=torch.hstack((v1,v2,emb_token))\n                else:\n                    v1=pred.hidden_states[-2][0][ids[0]+1].reshape(1,768)\n                    v2=pred.hidden_states[-2][0][ids[1]+1].reshape(1,768)\n                    v3=pred.hidden_states[-2][0][ids[2]+1].reshape(1,768)\n                    emb_token=torch.hstack((v1,v2,v3))\n\n            res1.append(emb_token)\n            res2.append(pred.pooler_output)\n        res1=torch.cat(res1)\n        res1=np.array(res1.detach().cpu())\n        res2=torch.cat(res2)\n        res2=np.array(res2.detach().cpu())\n        \n        return np.hstack((res1,res2))\n    \n    def preprocess(self):\n        self.clean()\n        self.df['synonyms'] = self.df['token'].apply(self.synonyms)\n        self.df['antonyms'] = self.df['token'].apply(self.antonyms)\n        #self.df['hypernyms'] = self.df['token'].apply(self.hypernyms)\n        self.df['hyponyms'] = self.df['token'].apply(self.hyponyms)\n        self.df['vowels']=self.vowels()\n        self.df['token_len']=self.token_len()\n        self.df['tf']=self.tf()\n        self.df['syllable']=self.syllable_count()\n        self.df=self.pos_tagger()\n        self.df['bert_token_count']=self.bert_no_of_tokens()\n        dummy=pd.get_dummies(self.df['corpus'],drop_first=True,prefix='corpus_')\n        df_y=self.df['complexity']\n        df_x=self.df.drop(columns=['corpus','id','sentence','token','complexity'])\n        df_x = pd.concat([df_x, dummy],axis=1)\n        np_vectors=np.hstack((self.glove_embedding(),self.contextual_features()))\n        temp_df=pd.DataFrame(np_vectors)\n        df_x=pd.concat([df_x,temp_df],axis=1)\n        return pd.concat([df_x,df_y],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T17:58:11.236853Z","iopub.execute_input":"2022-07-27T17:58:11.237257Z","iopub.status.idle":"2022-07-27T18:00:24.235738Z","shell.execute_reply.started":"2022-07-27T17:58:11.237225Z","shell.execute_reply":"2022-07-27T18:00:24.234470Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"single_trail=Features('../input/lcp-trail/lcp_single_trial.tsv').preprocess()\nsingle_trail.to_csv('single_trail.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:00:24.240733Z","iopub.execute_input":"2022-07-27T18:00:24.241151Z","iopub.status.idle":"2022-07-27T18:01:10.787497Z","shell.execute_reply.started":"2022-07-27T18:00:24.241114Z","shell.execute_reply":"2022-07-27T18:01:10.786198Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"single_train=Features('../input/lcp-train/lcp_single_train.tsv').preprocess()\nsingle_train.to_csv('single_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:01:56.417313Z","iopub.execute_input":"2022-07-27T18:01:56.417767Z","iopub.status.idle":"2022-07-27T18:17:05.821786Z","shell.execute_reply.started":"2022-07-27T18:01:56.417731Z","shell.execute_reply":"2022-07-27T18:17:05.820708Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"single_test=Features('../input/lcp-test/lcp_single_test.tsv').preprocess()\nsingle_test.to_csv('single_test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:17:05.825246Z","iopub.execute_input":"2022-07-27T18:17:05.825944Z","iopub.status.idle":"2022-07-27T18:18:56.058533Z","shell.execute_reply.started":"2022-07-27T18:17:05.825904Z","shell.execute_reply":"2022-07-27T18:18:56.057454Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#import necessary libraries\nfrom sklearn.metrics import r2_score\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\n\n#calculate the predicted scores and print it\ndef scores(actuall_labels, predicted_labels):\n    print('Pearson Score: ',pearsonr(actuall_labels,predicted_labels))\n    print('R2_Score: ',r2_score(actuall_labels,predicted_labels))\n    print('Spearmanr Score: ',spearmanr(actuall_labels,predicted_labels))\n    print('Mean Squared Error(MSE): ',mean_squared_error(actuall_labels,predicted_labels))\n    print('Mean Absolute Error(MAE): ',mean_absolute_error(actuall_labels,predicted_labels))\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:20.327625Z","iopub.execute_input":"2022-07-27T18:47:20.328153Z","iopub.status.idle":"2022-07-27T18:47:20.350148Z","shell.execute_reply.started":"2022-07-27T18:47:20.328101Z","shell.execute_reply":"2022-07-27T18:47:20.348523Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('./single_train.csv',index_col=[0])\ntest=pd.read_csv('./single_test.csv',index_col=[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:20.352796Z","iopub.execute_input":"2022-07-27T18:47:20.353687Z","iopub.status.idle":"2022-07-27T18:47:27.238581Z","shell.execute_reply.started":"2022-07-27T18:47:20.353637Z","shell.execute_reply":"2022-07-27T18:47:27.237311Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_x=train.drop(columns=['complexity'])\ntrain_y=train['complexity']","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:27.240444Z","iopub.execute_input":"2022-07-27T18:47:27.240786Z","iopub.status.idle":"2022-07-27T18:47:27.314250Z","shell.execute_reply.started":"2022-07-27T18:47:27.240756Z","shell.execute_reply":"2022-07-27T18:47:27.313244Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test_x=test.drop(columns=['complexity'])\ntest_y=test['complexity']","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:27.315501Z","iopub.execute_input":"2022-07-27T18:47:27.316651Z","iopub.status.idle":"2022-07-27T18:47:27.334724Z","shell.execute_reply.started":"2022-07-27T18:47:27.316609Z","shell.execute_reply":"2022-07-27T18:47:27.333242Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trainx=np.array(train_x)\ntrainy=np.array(train_y)\ntestx=np.array(test_x)\ntesty=np.array(test_y)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:27.337674Z","iopub.execute_input":"2022-07-27T18:47:27.338183Z","iopub.status.idle":"2022-07-27T18:47:27.511276Z","shell.execute_reply.started":"2022-07-27T18:47:27.338128Z","shell.execute_reply":"2022-07-27T18:47:27.509997Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbr =  GradientBoostingRegressor(random_state=19, n_estimators=100)\ngbr.fit(trainx, trainy)\npred4=gbr.predict(testx)\nscores(testy,pred4)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T18:47:27.513274Z","iopub.execute_input":"2022-07-27T18:47:27.513766Z","iopub.status.idle":"2022-07-27T18:56:12.274433Z","shell.execute_reply.started":"2022-07-27T18:47:27.513716Z","shell.execute_reply":"2022-07-27T18:56:12.273055Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Pearson Score:  (0.7398624271653934, 1.1029380343801114e-159)\nR2_Score:  0.5462761316286369\nSpearmanr Score:  SpearmanrResult(correlation=0.7100449178537698, pvalue=1.5367627681660304e-141)\nMean Squared Error(MSE):  0.0073435550333192864\nMean Absolute Error(MAE):  0.06569798774817318\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}